[TOC]
 

[SQL Server数据库大型应用解决方案总结](http://tech.it168.com/a2012/0110/1300/000001300144_all.shtml)
 
 
## 垂直拆分
 
按业务拆分，把数据表分别放到不同的数据库。
 
拆分方式实现起来比较简单，根据表名访问不同的数据库就可以了。
 

业务模块不明晰，耦合(表关联)度比较高的系统不适合使用这种拆分方式。

### 跨数据库的事务
分布式的事务恐怕要自己控制，自己写代码验证。

### 跨数据库的jion查询
只能避免jion，可能需要冗余之类的。
如果功能模块不是完全独立，则尽量减少其关联性，有时需要加冗余字段。
 
 
## 水平拆分
水平拆分：  将同一个表的数据进行分块保存到不同的数据库/表中，这些数据库中的表结构完全相同。
 一般水平分库是在垂直分库之后的。
 
需要解决的问题：数据路由、组装。


假如客户信息水平分布在不同的库中，那在统计客户总数、模糊查询客户的时候怎么处理呢？
水平划分的数据的规则要根据客户来划分。
或分页查找。类似这种客户总数的统计信息一致性要求不是那个强，因此可以离线查询或者单独保存。
 
划分规则只能满足绝大部分的业务需求

### 顺序拆分：
比如可以按订单的年份才分，2003年的放在db1中，2004年的db2,以此类推。当然也可以按主键拆分。
按号段分：
(1) user_id为区分，1～1000的对应DB1，1001～2000的对应DB2，以此类推；

 

**节点编码+自增**
每一个系统节点编码，比如说101 102 103 这种，然后一列自增列做为本地表唯一列，这一列加上之前人工维护的前缀就是全局唯一的了。


**最简单的方法**
比如把id为奇数的放到A库，为偶数的放B库。
比如3台数据库，第一台mysql主键从1开始每次加3，第二台从2开始每次加3，以此类推。。
 
 
 
2、搭建sequence server
 
     2.1、选用N台mysql作为sequence server，防止单点故障。
 
     2.2、每个server上的每张表都代表一个序列，每张表也只有一条记录（表级锁，选用myisam引擎）。
 
     2.3、第一台server的序列从1开始每次加N，第二台从2开始每次加N。
 
     2.4、获取时先从第一台server上获取nextVal并修改nextVal加N，如果第一台Server获取失败，则从第二台Server上获取。。


2 依赖数据库自增机制达到全局ID唯一
使用如下语句：
REPLACE INTO Tickets64 (stub) VALUES ('a'); 
SELECT LAST_INSERT_ID();
这样可以保证全局ID唯一，但这个Tickets64表依旧是个单点。

3 依赖数据库自增机制达到全局ID唯一并消除单点
在2的基础上，部署两个（多个）数据库实例，
设置自增步长为2（多个则为实例数），即auto-increment-increment = 2
设置auto-increment-offset分别为1,2.....
这样第一台数据库服务器的自增id为 1 3 5 7 9
第二台为2 4 6 8 10



4 解决每次请求全局ID都读库写库压力过大的问题
比如第一次启动业务服务，会请求一个唯一id为3559
如果是2、3的方法，则id为3559，这样每次都请求数据库，对数据库压力比较大
可以用3559 * 65536（举个例子，并不一定是65536）+ 内存自增变量来作为id
当内存自增变量到达65535时，从数据库重新获取一个自增id
这样即使有多台业务服务器，id也不会重复：
第一台 3559 * 65536 + 1,2,3.....65535
第二台 3560 * 65536 + 1,2,3.....65535
然后第一台到65535了，换一个数据库自增id，这时候可能是3561 * 65536 + 1,2,3....

我们目前采用4

20151122+我上面说的

方法四会导致ID的不连续，还有就是破坏了ID和时间的单调性（程序分布式部署的情况下）。

我们之前的订单号，是根据时间戳加计数器生成的，订单ID里面可以看出订单生成日期。



 
优点：可部分迁移
缺点：数据分布不均，可能2003年的订单有100W，2008年的有500W
 
 
### hash取模分
 
对user_id进行hash(或者如果user_id是数值型的话直接使用user_id的值也可)，然后用一个特定的数字对user_id的hash值进行取模运算
比如应用中需要将一个数据库切分成3个数据库的话，我们就用4这个数字对user_id的hash值进行取模运算，也就是user_id%4,这样的话每次运算就有四种可能：结果为1的时候对应DB1;结果为2的时候对应DB2;结果为0的时候对应DB3
 
优点：数据分布均匀
缺点：数据迁移的时候麻烦；不能按照机器性能分摊数据。
 

我说下通用的一些方法1、使用数据库自身的特性，在数据库启动参数里配置auto_increment_increment和 auto_increment_offset，我们不推荐这种方式，因为这会导致维护数据库的成本上升。2、配置一个单独的服务生成全局ID，可以是MySQL，也可以是NoSQL产品，甚至你可以构建自己的专门用来生成唯一ID的服务，为了提高效率，你可以批量获取唯一ID序列。3、另外一种方式是，通过函数、程序算法或者字段组合生成唯一ID，这种方式，可能有冲突，但是这个冲突的概率可以做到非常小，我们更推荐使用这种方式。
 


 
 
将user_id为1～10000的所有的文章信息放入DB1中 的article表中，将user_id为10001～20000的所有文章信息放入DB2中的article表中，以此类推，一直到DBn
 
 
 怎么划分，应该根据一定的规则，可以根据数据的产生者来做引导，上面的数据是由人产生的，可以根据人的id来划分数据库。然后再根据一定的规则，先获知数据在哪个数据库。
 




 
 
主要思路就是“怎么在短时间内生成多个ID不重复”。利用数据库写入时的自增能实现不重复，但是由于并发量过大会造成延时；将自增的量直接写在内存或缓存中能加快生成速度，但是可能会掉电丢失，因此可以定时写入数据库中或者文件中(写入时还要对比递增中的和库中的，取大者)；为了保险起见，防止上述情况失效，再在ID中加一个时间，现在一般也就精确到微秒吧，也就是时间(微秒级)+自增因子；如果为了应付更大访问，或者容灾，或者分布式，再加一个机器标志位，也就是机器标志+时间(微秒级)+自增因子；我们现在用这个方案来生成1000个/s左右并发量的ID，算是一个比较稳定安全的方案。当然，各个位置的长度要考虑数据的量，保存的时间等。如果像淘宝这种，肯定是用户ID+购物记录ID+时间等各个组合成一个购买记录的ID的。毕竟数据量太大了
 
 
 
 
 
唯一性
时间相关
粗略有序
可反解
可制造
下面我会分别讲每个作用后面的考虑和权衡，也会对比介绍一下业界已知的几种 ID 设计。
 
要唯一性，是否需要全局唯一？
 
说起全局唯一，通常大家都会在想到发号器服务，分布式的通常需要更大空间，中心式的则需要在一个合适的地方在会聚。这就可能涉及到锁，而锁意味着成本和性能的下降。所以当前的系统是否需要全局的唯一性，就是一个需要考虑的问题。
 
比如在通讯系统里，聊天消息可能就未必需要全局，因为一条消息只是某一个人发出，系统只要保证一个人维度的唯一性即可。本质上而言，这里利用了用户 ID 的唯一性，因为唯一性是可以依赖的，通常我们设计系统也都是基于类似的性质，比如后面降到的使用时间唯一性的方式。
 
用时间来做什么？千万年太久，只争朝夕？
 
前面说到唯一性可以依赖，我们需要选择的是依赖什么。通常的做法可以选择数据库自增，这在很多数据库里都是可以满足ACID 的操作。但是用数据库有个缺点，就是数据库有性能问题，在多机房情况下也很难处理。当然，你可以通过调整自增的步长来设计，但对于一个发号器而言，操作和维护都略重了。
 
而时间是天然唯一的，因此也是很多设计的选择。但对于一个8Byte的 ID 而言，时间并没有那么多。你如果精确到秒级别，三十年都要使用30bit，到毫秒级则要再增加10bit，你也只剩下20bit 可以做其他事情了。之所以在8Byte 上捣鼓，因为8Byte 是一个Long，不管在处理器和编译器还是语言层面，都是可以更好地被处理。
 
然而三十年够么？对于一个人来说，可能不够，但对一个系统而言，可能足够。我们经常开玩笑，互联网里能活三十年的系统有多少呢？三十年过去，你的系统可能都被重写 N 遍了。这样的信心同样来自于摩尔定律，三十年后，计算性能早就提高了上千倍，到时候更多Byte 都不会是问题了。
 
粗略有多粗略，秒还是毫秒？
 
每秒一个或者每毫秒一个ID明显是不够的，刚才说到还有20bit 可以做其他事情，就包括一个SequenceID。如果要达到精确的有序，就要对 Sequence 进行并发控制，性能上肯定会打折。所以经常会有的一个选择就是，在这个秒的级别上不再保证顺序，而整个 ID 则只保证时间上的有序。后一秒的 ID肯定比前一秒的大，但同一秒内可能后取的ID比前面的号小。这在使用时非常关键，你要理解，系统也要接受才可以。
 
那时间用秒还是毫秒呢？其实不用毫秒的时候就可以把空出来的10bit 送给 Sequence，但整个ID 的精度就下降了。峰值速度是更现实的考虑。Sequence 的空间决定了峰值的速度，而峰值也就意味着持续的时间不会太久。这方面，每秒100万比每毫秒1000限制更小。
 
可反解，解开的是什么？
 
一个 ID 生成之后，就会伴随着信息终身，排错分析的时候，我们需要查验。这时候一个可反解的 ID 可以帮上很多忙，从哪里来的，什么时候出生的。 跟身份证倒有点儿相通了，其实身份证就是一个典型的分布式 ID 生成器。
 
如果ID 里已经有了时间而且能解开，在存储层面可能不再需要timestamp 一类的字段了。微博的 ID 还有很多业务信息，这个后面会细讲。
 
可制造，为什么不用UUID？
 
互联网系统上可用性永远是优先指标。但由于分布式系统的脆弱，网络不稳定或者底层存储系统的不可用，业务系统随时面临着失败。为了给前端更友好的响应，我们需要能尽量容忍失败。比如在存储失败时，可能需要临时导出请求供后续处理，而后续处理时已经离开了当时的时间点，顺序跟其他系统错开了。我们需要制造出这样的ID 以便系统好像一直正常运行一样，可制造的 ID 让你可以控制生产日期（汗，有点儿假冒伪劣的意思了），然后继续下面的处理。
 
另一个重要场景就是数据清洗。这个属于较少遇到，但并不罕见的情况，可能是原来 ID 设计的不合理，也可能由于底层存储的改变，都可能出现。这样一个可制造的 ID 就会带来很多操作层面的便利。
 
这也是我们不用 UUID 的一个原因。UUID 标准可以保证在某时某地生成，但如果要控制生成一个特定时间的 UUID，可能需要底层库的改动。经验告诉我们，能在上层解决的问题不要透到下层，这种库的维护成本是非常高的。
 
 
 
 
 
设计细节
UUID 就不说了， 其他公开出来的这里说下SnowFlake、Weibo以及 Ticktick 的设计。
 
SnowFlake
 
41bit留给毫秒时间，10bit给MachineID，也就是机器要预先配置，剩下12位留给Sequence。代码虽然露出来了，但其实已经不可用了，据说是内部改造中。
 
Weibo
 
微博使用了秒级的时间，用了30bit，Sequence 用了15位，理论上可以搞定3.2w/s的速度。用4bit来区分IDC，也就是可以支持16个 IDC，对于核心机房来说够了。剩下的有2bit 用来区分业务，由于当前发号服务是机房中心式的，1bit 来区分热备。是的，也没有用满64bit。
 
Ticktick
 
也就是当前在环信系统里要用到的。使用了30bit 的秒级时间，20bit 给Sequence。这里是有个考虑，第一版实现还是希望到毫秒级，所以20bit 的前10bit给了毫秒来用，剩下10bit给 Sequence。等到峰值提高的时候可以暂时回到秒级。
 
前面说到的三十年问题，因此我在高位留了2bit 做 Version，或者到时候改造使用更长字节数，用第一位来标识不同 ID，或者可以把这2bit 挪给时间用，可以给系统改造留出一定的时间。
 
剩下的10bit 留给 MachineID，也就是说当前 ID 生成可以直接内嵌在业务服务中，最多支持千级别的服务器数量。最后有2bit 做Tag 用，可能区分群消息和单聊消息。同时你也看出，这个 ID 最多支持一天10亿消息，也是怕系统增速太快，这2bit 可以挪给 Sequence，可以支持40亿级别消息量，或者结合前面的版本支持到百亿级别。
 
修正：评论里指出上面一个计算错误，不挪借的话应该是支持一天约千亿级别。对比当前 Whatsapp 的600亿和腾讯 QQ 的200亿，已经足够了。
 
 
自己实现一个发号器非常简单，所以Ticktick 怎么实现并不重要。不过呐，我还是有 demo 源码的，见https://github.com/ericliang/ticktick


读写分离：对于时效性不高的数据，可以通过读写分离缓解数据库压力。需要解决的问题：在业务上区分哪些业务上是允许一定时间延迟的，以及数据同步问题。



 
        1、对记录数多的表我们进行拆分，那对与之相关联的一些表该怎么办？这个问题其实也是现实开发中比较普遍的一个问题，现在数据库表一般都会与其他表有关联的。有人会提出一个方法就是所有的表都不与其他表关联，至少在SQL执行层面上如此，这样不就解决了数据或业务关联问题了啊，但这里有个问题那就是如果都按照这样在SQL层面完全解耦，而在应用层面再关联的话，会导致数据库访问次数增加很多，而且网络传输数据增加，比如A表和B表是关联表，如果在SQL层面关联，则只执行一个SQL；如果在SQL层面独立，则需要执行两个SQL，分别查询出A表数据和B表数据，因为没有条件关联过滤，则数据肯定比执行关联SQL多很多，然后再在应用层进行关联。所以我个人觉得对性能要求高的系统中，还是需要使用SQL层面的关联的，但这里有一个原则肯定是要遵守，那就是不能让多个需要拆分表关联，因为这会导致拆分标准不一致而导致无法拆分。对关联SQL中的一个表需要拆分，其他都是相对静态的无需拆分的表，这种情况下的解决思路是将需拆分表拆分到多个库中，而静态表则同步到各个拆分库中。这里再上升一下，分析一下系统的表结构中，一般会分动态表（数据变化很大，数据量也可能很大的表）和静态表（数据变化很小的表，一般来说都是基础表，数据量也不会很大），将基础的静态表都放到一个公共库中，将动态表根据标准分拆到分库中，拆分完成后基础数据都在公共库维护，并同步到分库中，在分库中维护动态表，同时在查询时动态表可以与分库中静态表关联查询，这样就解决了这个问题。
        2、数据库表拆分的标准又是什么，按照什么来拆分？一般来说这个拆分标准可以按照数据范围分，比如1-100万一个表，100万-200万又是一个表；也可以按照时间顺序来拆分，比如一年的数据归到一张表中等；也可以按照地域范围来分，比如按照地市来分，每个或多个地市一个库等，反正这个个人觉得是按照具体的情况来分的，一般情况下，对带有较浓的分割标志的数据库表，可以根据分割标志来分割，对没有较浓分割标志的数据库表，则只能按照最笨的方法如数据范围来拆分了，有时候为了增加拆分质量，还可以先根据一个分割标志来分表，在根据另一个分割标志来分区等复合式的拆分方式来水平拆分数据库表。
        3、数据库表水平拆分后，访问数据库表的SQL必须要带上分割标志来确定目标数据库表，如果要对多个拆分数据库表进行查询，则需要通过多次访问数据库表来完成，同时在应用层面将数据合并来做到。但有时候，一般需要可以通过至少两种方式（或分割标志）来获取目标数据库表。举个例子，大型网络游戏中因为玩家太多了（比如达到几千万甚至亿级别时），所以将玩家的用户信息分库分表，当用户登录时，现在一般的做法都是会让用户自己选择是哪一区的，根据这个选择来确定目标数据库表，但如果我们改一下，用户不知道自己是哪一区的，只知道自己的用户编号，输入用户编号后需要由系统自动根据用户编号来路由到目标数据库表。在这种情况下，个人觉得需要有一个规则来保证用户编号的规律性，比如可以在用户申请时，针对选择的不同区来生成不同的用户编号，比如1区是aaa+8位的顺序编号，2区是bbb+8位的顺序编号，这样的话，对aaa，bbb之类的分类编号是可以通过数据库表来管理的，比如用户编号是aaa开头和abc开头的都是1区的用户这样的规则就可以管理起来，这样当用户输入用户编号时，系统通过截取用户编号前三位，并到数据库表中查询出这前三位对应的哪个区，这样就可以获得这个用户的目标数据库表了。等到查询出这个用户信息后，这个用户信息中必定会存在分割标志信息的（这个例子中就是属于哪个区的），对这个用户信息缓存，就不再需要使用之前那种方式来确定目标数据库表了，而只需要根据缓存的用户信息中的属于哪个区的信息就可以来确定目标数据库表了。分析这两种确定目标数据库表的方式，一般来说前一种方式比较复杂，性能上消耗也较多，这种方式只有在第二种方式无法判断的情况下使用，所以使用频率相对来说非常低，而第二种方式则相对简单，而且性能也很好，这种方式是默认使用方式，使用频率相对来说非常高，但有时候因为信息不全无法使用第二种方式，所以必须要有前一种方式来补充使用。
        4、数据库水平拆分成多个库时，这时有一个事情是必须会碰到的，那就是数据库的连接。一般来说，应用服务器或应用系统对数据库连接的管理一般会通过连接池来管理，这样可以大大提高效率，而不会使用动态连接。这里就出来一个问题，当一个数据库水平拆分成多个数据库时，必然数据库连接池也会增加到多个，在一定范围内应该是不会有问题的，但毕竟应用服务器的性能也是有上限的，当数据库水平拆分成N个数据库时，应用服务器的性能就会吃不消了，这时候需要对应用服务器进行扩展了，比如一台应用服务器对应几个数据库连接等，当然这是比较深入的事情了，这里只把问题抛出来，不再多说。
        5、说到这里要说一下应用设计开发上的问题，首先是对目标数据库表路由模块必须要独立，如果路由不独立出来，那以后万一路由策略变更的话会死的很惨，而且路由算法是一个典型的策略模式应用，最好能实现成策略模式，以方便以后路由策略变更时应用可以无缝的切换路由策略。其次是对开发来说，最好能使用ibatis之类的持久层，既有一定的封装，也可以将SQL独立配置，这样在开发人员开发时，可以灵活的写SQL来实现逻辑，也可以对SQL语句进行管理，同时DBA可以很方便的对这些SQL进行专业的优化，而与应用开发无关。现在一些先行者已经在努力实现将数据库拆分的影响封装在代理中的项目，比如变形虫项目，这些项目的出现将会使数据库拆分后对应用开发的影响越来越小。最后一个是事务，如果可以接受分布式事务的性能那当然是最好的；如果不能接受，那一般的做法就是事务补偿（指在同一业务操作中当事务A提交，但事务B发生错误回滚后，为保持操作一致性和数据正确性，必须要做事务A操作的反操作来补偿事务A的提交，消除事务A的提交对数据结果的影响），但事务补偿会增加开发的工作量等问题；或者不是非常重要的业务操作时，通过保证事务B执行的成功率（比如先进行查询或预执行操作），从而使事务B的失败率下降到可以忽略的程度，从而可以不考虑事务的问题。
        6、还有一个非常重要的需要说一下，一般来说很多都是对原有系统的改造，这样的话就必然会有需要对原有数据的处理割接，这块工作也是非常重要的，数据库拆分方案做得最好，如果原有数据不能无缝的割接到新的拆分后的数据库中的话，那都是白搭。另外还有业务层面的问题，比如数据库表拆分引起的业务流程更改，业务操作习惯更改等方面的问题也要提早考虑和解决。
        总之，数据库表水平拆分是非常复杂的，需要综合各个方面考虑完善，套用网友cauherk的说法“系统的切分是个很复杂的技术活，要综合考虑，而不仅仅从数据库层面考虑。业务的使用、分库的原则、数据的割接、开发的侵入、可操作的易难程度、后期的管理等等都是需要考虑的因素。”。
 
 